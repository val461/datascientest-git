{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "589e2234-16bc-4571-adfb-08dffd97e9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, 0.0, 1.0]\n",
      "[0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Defining a custom error\n",
    "\n",
    "class NotFittedError(Exception):\n",
    "    pass\n",
    "\n",
    "# Defining the desired class\n",
    "\n",
    "class StandardScaler:\n",
    "    mean = None\n",
    "    def fit(self, iterable):\n",
    "        self.mean = sum(iterable)/len(iterable)\n",
    "    def transform(self, iterable):\n",
    "        if self.mean != None:\n",
    "            return([element - self.mean for element in iterable])\n",
    "        else:\n",
    "            raise NotFittedError(\"You first have to fit the estimator!\")\n",
    "    def fit_transform(self, iterable):\n",
    "        self.mean = sum(iterable)/len(iterable)\n",
    "        return([element - self.mean for element in iterable])\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit([1,2,3])\n",
    "print(sc.transform([1,2,3])) # Testing that the resulting list has a 0 mean\n",
    "print(sc.fit_transform([1,1,1])) # Testing that the output list has a 0 mean that that the mean was learned\n",
    "print(sc.transform([2,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86016e1-2019-4733-af8d-657738f55000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1379995e-2d54-4a47-b6e3-6d018c033c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  A|  B|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  4|\n",
      "|  5|  6|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Cours de Spark\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.parallelize([(1,2), (3,4), (5,6)])\n",
    "df = rdd.toDF([\"A\", \"B\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26060fa9-957d-4640-8555-6fb5ee95f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: long (nullable = true)\n",
      " |-- B: long (nullable = true)\n",
      " |-- C: vector (nullable = true)\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'> None\n",
      "+---+---+---------+\n",
      "|  A|  B|        C|\n",
      "+---+---+---------+\n",
      "|  1|  2|[1.0,2.0]|\n",
      "|  3|  4|[3.0,4.0]|\n",
      "|  5|  6|[5.0,6.0]|\n",
      "+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "va = VectorAssembler(inputCols=[\"A\", \"B\"], outputCol=\"C\")\n",
    "new_df = va.transform(df)\n",
    "print(type(new_df), new_df.printSchema())\n",
    "new_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b04a6d5-1df2-4e0f-b52b-f3870f4d8742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  2|  4|  6|\n",
      "|  4|  8| 10|\n",
      "+---+---+---+\n",
      "\n",
      "+---+---+---+--------------+\n",
      "|  A|  B|  C|             D|\n",
      "+---+---+---+--------------+\n",
      "|  1|  2|  3| [1.0,2.0,3.0]|\n",
      "|  2|  4|  6| [2.0,4.0,6.0]|\n",
      "|  4|  8| 10|[4.0,8.0,10.0]|\n",
      "+---+---+---+--------------+\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------------------+\n",
      "|          pearson(D)|\n",
      "+--------------------+\n",
      "|1.0              ...|\n",
      "+--------------------+\n",
      "\n",
      "##\n",
      "\n",
      " DenseMatrix([[1.        , 1.        , 0.99419163],\n",
      "             [1.        , 1.        , 0.99419163],\n",
      "             [0.99419163, 0.99419163, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "rdd = sc.parallelize([(1,2,3), (2,4,6), (4,8,10)])\n",
    "df = rdd.toDF([\"A\", \"B\", \"C\"])\n",
    "df.show()\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"A\", \"B\", \"C\"], outputCol=\"D\")\n",
    "new_df = va.transform(df)\n",
    "new_df.show()\n",
    "\n",
    "Corr = Correlation.corr(new_df, \"D\", \"pearson\")\n",
    "print(type(Corr))\n",
    "Corr.show()\n",
    "print('##\\n\\n',str(Corr.collect()[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd489297-8d03-4251-a178-d79230b5a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------+\n",
      "|aggregate_metrics(features, 1.0)                                                          |\n",
      "+------------------------------------------------------------------------------------------+\n",
      "|{[0.0,0.0,3.0], [1.3333333333333333,4.0,6.333333333333334], [4.0,8.0,10.0], [1.0,2.0,3.0]}|\n",
      "+------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics(\"min\", \"mean\", \"max\", \"numNonZeros\")\n",
    "\n",
    "rdd = sc.parallelize([(0,0,3), (0,4,6), (4,8,10)])\n",
    "df = rdd.toDF([\"A\", \"B\", \"C\"])\n",
    "va = VectorAssembler(inputCols=[\"A\", \"B\", \"C\"], outputCol=\"features\")\n",
    "new_df = va.transform(df)\n",
    "\n",
    "new_df.select(summarizer.summary(new_df[\"features\"])).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31c8b12b-896a-43f4-bfb8-deefeababd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  0|  0|  3|\n",
      "|  0|  4|  6|\n",
      "|  4|  8| 10|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f1cfd1e-6f68-4272-af4d-8c0f9752632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species: string (nullable = true)\n",
      " |-- island: string (nullable = true)\n",
      " |-- bill_length_mm: double (nullable = true)\n",
      " |-- bill_depth_mm: double (nullable = true)\n",
      " |-- flipper_length_mm: integer (nullable = true)\n",
      " |-- body_mass_g: integer (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  MALE|\n",
      "| Adelie|Torgersen|          39.5|         17.4|              186|       3800|FEMALE|\n",
      "| Adelie|Torgersen|          40.3|         18.0|              195|       3250|FEMALE|\n",
      "| Adelie|Torgersen|          NULL|         NULL|             NULL|       NULL|  NULL|\n",
      "| Adelie|Torgersen|          36.7|         19.3|              193|       3450|FEMALE|\n",
      "| Adelie|Torgersen|          39.3|         20.6|              190|       3650|  MALE|\n",
      "| Adelie|Torgersen|          38.9|         17.8|              181|       3625|FEMALE|\n",
      "| Adelie|Torgersen|          39.2|         19.6|              195|       4675|  MALE|\n",
      "| Adelie|Torgersen|          34.1|         18.1|              193|       3475|  NULL|\n",
      "| Adelie|Torgersen|          42.0|         20.2|              190|       4250|  NULL|\n",
      "| Adelie|Torgersen|          37.8|         17.1|              186|       3300|  NULL|\n",
      "| Adelie|Torgersen|          37.8|         17.3|              180|       3700|  NULL|\n",
      "| Adelie|Torgersen|          41.1|         17.6|              182|       3200|FEMALE|\n",
      "| Adelie|Torgersen|          38.6|         21.2|              191|       3800|  MALE|\n",
      "| Adelie|Torgersen|          34.6|         21.1|              198|       4400|  MALE|\n",
      "| Adelie|Torgersen|          36.6|         17.8|              185|       3700|FEMALE|\n",
      "| Adelie|Torgersen|          38.7|         19.0|              195|       3450|FEMALE|\n",
      "| Adelie|Torgersen|          42.5|         20.7|              197|       4500|  MALE|\n",
      "| Adelie|Torgersen|          34.4|         18.4|              184|       3325|FEMALE|\n",
      "| Adelie|Torgersen|          46.0|         21.5|              194|       4200|  MALE|\n",
      "+-------+---------+--------------+-------------+-----------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins_df = spark.read.option(\"header\", True)\\\n",
    "                        .option(\"inferSchema\", True)\\\n",
    "                        .option(\"escape\", \"\\\"\")\\\n",
    "                        .csv(\"penguins.csv\")\n",
    "\n",
    "print(penguins_df.printSchema())\n",
    "penguins_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "521340c5-41af-4998-ad73-5c3eb402c44e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 68:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+---------+-----------------+------------------+------------------+-----------------+------+\n",
      "|summary|species|   island|   bill_length_mm|     bill_depth_mm| flipper_length_mm|      body_mass_g|   sex|\n",
      "+-------+-------+---------+-----------------+------------------+------------------+-----------------+------+\n",
      "|  count|    344|      344|              342|               342|               342|              342|   333|\n",
      "|   mean|   NULL|     NULL|43.92192982456142|17.151169590643278|200.91520467836258|4201.754385964912|  NULL|\n",
      "| stddev|   NULL|     NULL|5.459583713926537|1.9747931568167807|14.061713679356952|801.9545356980949|  NULL|\n",
      "|    min| Adelie|   Biscoe|             32.1|              13.1|               172|             2700|FEMALE|\n",
      "|    25%|   NULL|     NULL|             39.2|              15.6|               190|             3550|  NULL|\n",
      "|    50%|   NULL|     NULL|             44.4|              17.3|               197|             4050|  NULL|\n",
      "|    75%|   NULL|     NULL|             48.5|              18.7|               213|             4750|  NULL|\n",
      "|    max| Gentoo|Torgersen|             59.6|              21.5|               231|             6300|  MALE|\n",
      "+-------+-------+---------+-----------------+------------------+------------------+-----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "penguins_df.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abdf8c66-c626-4d92-bea7-ce53959c399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+-------------+\n",
      "| bill_length_mm_f|   bill_depth_mm_f|flipper_length_mm_f|body_mass_g_f|\n",
      "+-----------------+------------------+-------------------+-------------+\n",
      "|             39.1|              18.7|                181|         3750|\n",
      "|             39.5|              17.4|                186|         3800|\n",
      "|             40.3|              18.0|                195|         3250|\n",
      "|43.92192982456142|17.151169590643278|                200|         4050|\n",
      "|             36.7|              19.3|                193|         3450|\n",
      "|             39.3|              20.6|                190|         3650|\n",
      "|             38.9|              17.8|                181|         3625|\n",
      "|             39.2|              19.6|                195|         4675|\n",
      "|             34.1|              18.1|                193|         3475|\n",
      "|             42.0|              20.2|                190|         4250|\n",
      "|             37.8|              17.1|                186|         3300|\n",
      "|             37.8|              17.3|                180|         3700|\n",
      "|             41.1|              17.6|                182|         3200|\n",
      "|             38.6|              21.2|                191|         3800|\n",
      "|             34.6|              21.1|                198|         4400|\n",
      "|             36.6|              17.8|                185|         3700|\n",
      "|             38.7|              19.0|                195|         3450|\n",
      "|             42.5|              20.7|                197|         4500|\n",
      "|             34.4|              18.4|                184|         3325|\n",
      "|             46.0|              21.5|                194|         4200|\n",
      "+-----------------+------------------+-------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "impute_mean = Imputer(strategy=\"mean\",\\\n",
    "                      inputCols=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"],\\\n",
    "                      outputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\"])\n",
    "\n",
    "impute_median = Imputer(strategy=\"median\",\\\n",
    "                        inputCol=\"body_mass_g\",\\\n",
    "                        outputCol=\"body_mass_g_f\")\n",
    "\n",
    "model_mean = impute_mean.fit(penguins_df)\n",
    "model_median = impute_median.fit(penguins_df)\n",
    "\n",
    "penguins_no_na = model_mean.transform(model_median.transform(penguins_df))\n",
    "penguins_no_na.select([\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\", \"body_mass_g_f\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "879e03a9-2d96-461a-bf29-d68fd54899bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+--------+\n",
      "|label|sex_f|island_f|\n",
      "+-----+-----+--------+\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  2.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  2.0|     2.0|\n",
      "|  0.0|  2.0|     2.0|\n",
      "|  0.0|  2.0|     2.0|\n",
      "|  0.0|  2.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "|  0.0|  1.0|     2.0|\n",
      "|  0.0|  0.0|     2.0|\n",
      "+-----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "index = StringIndexer(inputCols=[\"species\", \"sex\", \"island\"],\\\n",
    "                      outputCols=[\"label\", \"sex_f\", \"island_f\"],\\\n",
    "                      handleInvalid=\"keep\")\n",
    "\n",
    "model_index = index.fit(penguins_no_na)\n",
    "\n",
    "penguins_indexed = model_index.transform(penguins_no_na)\n",
    "penguins_indexed.select([\"label\", \"sex_f\", \"island_f\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f80d8ed1-ed52-40cd-b34d-3dc2ca085ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|sex_f|\n",
      "+------+-----+\n",
      "|  MALE|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|FEMALE|  1.0|\n",
      "|  NULL|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|  MALE|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|  MALE|  0.0|\n",
      "|  NULL|  0.0|\n",
      "|  NULL|  0.0|\n",
      "|  NULL|  0.0|\n",
      "|  NULL|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|  MALE|  0.0|\n",
      "|  MALE|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|FEMALE|  1.0|\n",
      "|  MALE|  0.0|\n",
      "|FEMALE|  1.0|\n",
      "|  MALE|  0.0|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "impute_mode = Imputer(strategy=\"mode\",\\\n",
    "                      missingValue=2.0,\\\n",
    "                      inputCol=\"sex_f\",\\\n",
    "                      outputCol=\"sex_f\")\n",
    "\n",
    "model_mode = impute_mode.fit(penguins_indexed)\n",
    "\n",
    "penguins_indexed_f = model_mode.transform(penguins_indexed)\n",
    "penguins_indexed_f.select(\"sex\", \"sex_f\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b58e9251-8657-4bc8-bfe3-d679a5be9179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|island_encoded|\n",
      "+--------------+\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "| (3,[2],[1.0])|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"island_f\",\\\n",
    "                        outputCol=\"island_encoded\")\n",
    "\n",
    "model_encoder = encoder.fit(penguins_indexed_f)\n",
    "\n",
    "penguins_encoded = model_encoder.transform(penguins_indexed_f)\n",
    "penguins_encoded.select(\"island_encoded\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a2e3abb-a6a9-4700-89bb-1a628e2219e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|  numerical_features|\n",
      "+--------------------+\n",
      "|[39.1,18.7,181.0,...|\n",
      "|[39.5,17.4,186.0,...|\n",
      "|[40.3,18.0,195.0,...|\n",
      "|[43.9219298245614...|\n",
      "|[36.7,19.3,193.0,...|\n",
      "|[39.3,20.6,190.0,...|\n",
      "|[38.9,17.8,181.0,...|\n",
      "|[39.2,19.6,195.0,...|\n",
      "|[34.1,18.1,193.0,...|\n",
      "|[42.0,20.2,190.0,...|\n",
      "|[37.8,17.1,186.0,...|\n",
      "|[37.8,17.3,180.0,...|\n",
      "|[41.1,17.6,182.0,...|\n",
      "|[38.6,21.2,191.0,...|\n",
      "|[34.6,21.1,198.0,...|\n",
      "|[36.6,17.8,185.0,...|\n",
      "|[38.7,19.0,195.0,...|\n",
      "|[42.5,20.7,197.0,...|\n",
      "|[34.4,18.4,184.0,...|\n",
      "|[46.0,21.5,194.0,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|  numerical_features|     scaled_features|\n",
      "+--------------------+--------------------+\n",
      "|[39.1,18.7,181.0,...|[7.18268959235864...|\n",
      "|[39.5,17.4,186.0,...|[7.25616979279197...|\n",
      "|[40.3,18.0,195.0,...|[7.40313019365864...|\n",
      "|[43.9219298245614...|[8.06848051731928...|\n",
      "|[36.7,19.3,193.0,...|[6.74180838975862...|\n",
      "|[39.3,20.6,190.0,...|[7.21942969257530...|\n",
      "|[38.9,17.8,181.0,...|[7.14594949214197...|\n",
      "|[39.2,19.6,195.0,...|[7.20105964246697...|\n",
      "|[34.1,18.1,193.0,...|[6.26418708694193...|\n",
      "|[42.0,20.2,190.0,...|[7.71542104550033...|\n",
      "|[37.8,17.1,186.0,...|[6.94387894095029...|\n",
      "|[37.8,17.3,180.0,...|[6.94387894095029...|\n",
      "|[41.1,17.6,182.0,...|[7.55009059452532...|\n",
      "|[38.6,21.2,191.0,...|[7.09083934181697...|\n",
      "|[34.6,21.1,198.0,...|[6.35603733748360...|\n",
      "|[36.6,17.8,185.0,...|[6.72343833965028...|\n",
      "|[38.7,19.0,195.0,...|[7.10920939192530...|\n",
      "|[42.5,20.7,197.0,...|[7.807271296042,1...|\n",
      "|[34.4,18.4,184.0,...|[6.31929723726693...|\n",
      "|[46.0,21.5,194.0,...|[8.45022304983369...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\", \"body_mass_g_f\"],\\\n",
    "                     outputCol=\"numerical_features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "penguins_scaled_ass = va.transform(penguins_encoded)\n",
    "penguins_scaled_ass.select(\"numerical_features\").show()\n",
    "\n",
    "scalerModel = scaler.fit(penguins_scaled_ass)\n",
    "\n",
    "penguins_scaled = scalerModel.transform(penguins_scaled_ass)\n",
    "penguins_scaled.select(\"numerical_features\", \"scaled_features\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52f78590-433e-46e8-8a4f-a5718dd6a8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|numerical_features                                 |scaled_features                                                             |\n",
      "+---------------------------------------------------+----------------------------------------------------------------------------+\n",
      "|[39.1,18.7,181.0,3750.0]                           |[7.18268959235864,9.497074906915868,12.909363540496079,4.689278807334335]   |\n",
      "|[39.5,17.4,186.0,3800.0]                           |[7.256169792791977,8.836850448146317,13.26597579299597,4.751802524765459]   |\n",
      "|[40.3,18.0,195.0,3250.0]                           |[7.403130193658649,9.14156942911688,13.907877847495774,4.06404163302309]    |\n",
      "|[43.92192982456142,17.151169590643278,200.0,4050.0]|[8.06848051731928,8.710478200190202,14.264490099995667,5.064421111921082]   |\n",
      "|[36.7,19.3,193.0,3450.0]                           |[6.741808389758622,9.801793887886433,13.765232946495818,4.314136502747588]  |\n",
      "|[39.3,20.6,190.0,3650.0]                           |[7.219429692575308,10.462018346655984,13.551265594995883,4.5642313724720855]|\n",
      "|[38.9,17.8,181.0,3625.0]                           |[7.1459494921419715,9.039996435460026,12.909363540496079,4.532969513756524] |\n",
      "|[39.2,19.6,195.0,4675.0]                           |[7.201059642466975,9.954153378371714,13.907877847495774,5.845967579810138]  |\n",
      "|[34.1,18.1,193.0,3475.0]                           |[6.264187086941934,9.192355925945307,13.765232946495818,4.34539836146315]   |\n",
      "|[42.0,20.2,190.0,4250.0]                           |[7.71542104550033,10.258872359342275,13.551265594995883,5.314515981645579]  |\n",
      "|[37.8,17.1,186.0,3300.0]                           |[6.943878940950296,8.684490957661037,13.26597579299597,4.126565350454214]   |\n",
      "|[37.8,17.3,180.0,3700.0]                           |[6.943878940950296,8.78606395131789,12.8380410899961,4.62675508990321]      |\n",
      "|[41.1,17.6,182.0,3200.0]                           |[7.550090594525323,8.938423441803172,12.980685990996056,4.001517915591966]  |\n",
      "|[38.6,21.2,191.0,3800.0]                           |[7.09083934181697,10.766737327626547,13.622588045495862,4.751802524765459]  |\n",
      "|[34.6,21.1,198.0,4400.0]                           |[6.356037337483605,10.715950830798121,14.12184519899571,5.502087133938953]  |\n",
      "|[36.6,17.8,185.0,3700.0]                           |[6.723438339650287,9.039996435460026,13.194653342495991,4.62675508990321]   |\n",
      "|[38.7,19.0,195.0,3450.0]                           |[7.1092093919253045,9.649434397401151,13.907877847495774,4.314136502747588] |\n",
      "|[42.5,20.7,197.0,4500.0]                           |[7.807271296042,10.51280484348441,14.050522748495732,5.6271345688012016]    |\n",
      "|[34.4,18.4,184.0,3325.0]                           |[6.3192972372669365,9.344715416430587,13.123330891996012,4.157827209169777] |\n",
      "|[46.0,21.5,194.0,4200.0]                           |[8.450223049833694,10.919096818111829,13.836555396995797,5.251992264214455] |\n",
      "+---------------------------------------------------+----------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins_scaled.select(\"numerical_features\", \"scaled_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "903bbbe2-85f1-4066-bb4a-b724ad407224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[0.0,0.0,1.0,7.18...|  0.0|\n",
      "|[0.0,0.0,1.0,7.25...|  0.0|\n",
      "|[0.0,0.0,1.0,7.40...|  0.0|\n",
      "|[0.0,0.0,1.0,8.06...|  0.0|\n",
      "|[0.0,0.0,1.0,6.74...|  0.0|\n",
      "|[0.0,0.0,1.0,7.21...|  0.0|\n",
      "|[0.0,0.0,1.0,7.14...|  0.0|\n",
      "|[0.0,0.0,1.0,7.20...|  0.0|\n",
      "|[0.0,0.0,1.0,6.26...|  0.0|\n",
      "|[0.0,0.0,1.0,7.71...|  0.0|\n",
      "|[0.0,0.0,1.0,6.94...|  0.0|\n",
      "|[0.0,0.0,1.0,6.94...|  0.0|\n",
      "|[0.0,0.0,1.0,7.55...|  0.0|\n",
      "|[0.0,0.0,1.0,7.09...|  0.0|\n",
      "|[0.0,0.0,1.0,6.35...|  0.0|\n",
      "|[0.0,0.0,1.0,6.72...|  0.0|\n",
      "|[0.0,0.0,1.0,7.10...|  0.0|\n",
      "|[0.0,0.0,1.0,7.80...|  0.0|\n",
      "|[0.0,0.0,1.0,6.31...|  0.0|\n",
      "|[0.0,0.0,1.0,8.45...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_va = VectorAssembler(inputCols=[\"island_encoded\", \"scaled_features\", \"sex_f\"],\\\n",
    "                           outputCol=\"features\")\n",
    "\n",
    "penguins_final = final_va.transform(penguins_scaled).select(\"features\", \"label\")\n",
    "penguins_final.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "007429d9-eac9-40d6-b212-999d8cd279d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                    |label|\n",
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "|[0.0,0.0,1.0,7.18268959235864,9.497074906915868,12.909363540496079,4.689278807334335,0.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,7.256169792791977,8.836850448146317,13.26597579299597,4.751802524765459,1.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,7.403130193658649,9.14156942911688,13.907877847495774,4.06404163302309,1.0]    |0.0  |\n",
      "|[0.0,0.0,1.0,8.06848051731928,8.710478200190202,14.264490099995667,5.064421111921082,0.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,6.741808389758622,9.801793887886433,13.765232946495818,4.314136502747588,1.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,7.219429692575308,10.462018346655984,13.551265594995883,4.5642313724720855,0.0]|0.0  |\n",
      "|[0.0,0.0,1.0,7.1459494921419715,9.039996435460026,12.909363540496079,4.532969513756524,1.0] |0.0  |\n",
      "|[0.0,0.0,1.0,7.201059642466975,9.954153378371714,13.907877847495774,5.845967579810138,0.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,6.264187086941934,9.192355925945307,13.765232946495818,4.34539836146315,0.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,7.71542104550033,10.258872359342275,13.551265594995883,5.314515981645579,0.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,6.943878940950296,8.684490957661037,13.26597579299597,4.126565350454214,0.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,6.943878940950296,8.78606395131789,12.8380410899961,4.62675508990321,0.0]      |0.0  |\n",
      "|[0.0,0.0,1.0,7.550090594525323,8.938423441803172,12.980685990996056,4.001517915591966,1.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,7.09083934181697,10.766737327626547,13.622588045495862,4.751802524765459,0.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,6.356037337483605,10.715950830798121,14.12184519899571,5.502087133938953,0.0]  |0.0  |\n",
      "|[0.0,0.0,1.0,6.723438339650287,9.039996435460026,13.194653342495991,4.62675508990321,1.0]   |0.0  |\n",
      "|[0.0,0.0,1.0,7.1092093919253045,9.649434397401151,13.907877847495774,4.314136502747588,1.0] |0.0  |\n",
      "|[0.0,0.0,1.0,7.807271296042,10.51280484348441,14.050522748495732,5.6271345688012016,0.0]    |0.0  |\n",
      "|[0.0,0.0,1.0,6.3192972372669365,9.344715416430587,13.123330891996012,4.157827209169777,1.0] |0.0  |\n",
      "|[0.0,0.0,1.0,8.450223049833694,10.919096818111829,13.836555396995797,5.251992264214455,0.0] |0.0  |\n",
      "+--------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc32bb49-c0bc-494f-a4d9-74f5bbe0a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "lrModel = lr.fit(penguins_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b9b7394-8d9a-4e1d-98c3-f23fd1ea71db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[0.0,0.0,1.0,7.18...|  0.0|[1.58875008949583...|[0.63772122064009...|       0.0|\n",
      "|[0.0,0.0,1.0,7.25...|  0.0|[1.55648011267446...|[0.59867134949241...|       0.0|\n",
      "|[0.0,0.0,1.0,7.40...|  0.0|[1.49194015903172...|[0.57846427961242...|       0.0|\n",
      "|[0.0,0.0,1.0,8.06...|  0.0|[1.19974118031367...|[0.46883296562379...|       0.0|\n",
      "|[0.0,0.0,1.0,6.74...|  0.0|[1.78236995042404...|[0.67034990632772...|       0.0|\n",
      "|[0.0,0.0,1.0,7.21...|  0.0|[1.57261510108515...|[0.64553243203912...|       0.0|\n",
      "|[0.0,0.0,1.0,7.14...|  0.0|[1.60488507790651...|[0.62875276475007...|       0.0|\n",
      "|[0.0,0.0,1.0,7.20...|  0.0|[1.58068259529049...|[0.61526950047541...|       0.0|\n",
      "|[0.0,0.0,1.0,6.26...|  0.0|[1.99212479976294...|[0.69727180569744...|       0.0|\n",
      "|[0.0,0.0,1.0,7.71...|  0.0|[1.35479275754091...|[0.58397160672895...|       0.0|\n",
      "|[0.0,0.0,1.0,6.94...|  0.0|[1.69362751416528...|[0.63026875318836...|       0.0|\n",
      "|[0.0,0.0,1.0,6.94...|  0.0|[1.69362751416528...|[0.64275960872130...|       0.0|\n",
      "|[0.0,0.0,1.0,7.55...|  0.0|[1.42740020538899...|[0.58460023474654...|       0.0|\n",
      "|[0.0,0.0,1.0,7.09...|  0.0|[1.62908756052254...|[0.66354385277448...|       0.0|\n",
      "|[0.0,0.0,1.0,6.35...|  0.0|[1.95178732873622...|[0.71601457542101...|       0.0|\n",
      "|[0.0,0.0,1.0,6.72...|  0.0|[1.79043744462938...|[0.66267123388857...|       0.0|\n",
      "|[0.0,0.0,1.0,7.10...|  0.0|[1.62102006631720...|[0.62495718408824...|       0.0|\n",
      "|[0.0,0.0,1.0,7.80...|  0.0|[1.31445528651420...|[0.56625212219215...|       0.0|\n",
      "|[0.0,0.0,1.0,6.31...|  0.0|[1.96792231714691...|[0.71396771507550...|       0.0|\n",
      "|[0.0,0.0,1.0,8.45...|  0.0|[1.03209298932722...|[0.51653815299054...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "penguins_predict = lrModel.transform(penguins_final)\n",
    "penguins_predict.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36549593-5c4f-4941-b803-620d03fe8aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.730188079806837\n",
      "0.8052325581395349\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "print(evaluator.evaluate(penguins_predict))\n",
    "print(evaluator.evaluate(penguins_predict, {evaluator.metricName: \"accuracy\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eeb23469-6cc5-430d-8cbc-21e0439ee58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6483238016339061\n",
      "0.7472527472527473\n"
     ]
    }
   ],
   "source": [
    "train, test = penguins_final.randomSplit([0.7, 0.3], seed=12345)\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "test_predict = lrModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "print(evaluator.evaluate(test_predict))\n",
    "print(evaluator.evaluate(test_predict, {evaluator.metricName: \"accuracy\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3aa11a03-909e-426c-9627-686308b97168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f1'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.getMetricName()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f12cc1d7-9e01-4aba-9ad3-bb29f24b66a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta: The beta value used in weightedFMeasure|fMeasureByLabel. Must be > 0. The default value is 1. (default: 1.0)\n",
      "eps: log-loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)). Must be in range (0, 0.5). The default value is 1e-15. (default: 1e-15)\n",
      "labelCol: label column name. (default: label)\n",
      "metricLabel: The class whose metric will be computed in truePositiveRateByLabel|falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel. Must be >= 0. The default value is 0. (default: 0.0)\n",
      "metricName: metric name in evaluation (f1|accuracy|weightedPrecision|weightedRecall|weightedTruePositiveRate| weightedFalsePositiveRate|weightedFMeasure|truePositiveRateByLabel| falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel| logLoss|hammingLoss) (default: f1)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce4d7f6e-6b50-4698-93a5-1b0360a9913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "lr = LogisticRegression()\n",
    "output = ParamGridBuilder().baseOn({lr.labelCol: 'label'})\\\n",
    "                           .baseOn([lr.predictionCol, 'prediction'])\\\n",
    "                           .addGrid(lr.regParam, [1.0, 2.0])\\\n",
    "                           .addGrid(lr.maxIter, [1, 5])\\\n",
    "                           .build()\n",
    "\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=lr,\\\n",
    "                    estimatorParamMaps=output,\\\n",
    "                    evaluator=evaluator,\\\n",
    "                    numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29cadb6d-e58a-4124-a157-cde65a95ab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6684941965664856\n",
      "0.7472527472527473\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(train)\n",
    "\n",
    "test_predict = cvModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "print(evaluator.evaluate(test_predict))\n",
    "print(evaluator.evaluate(test_predict, {evaluator.metricName: \"accuracy\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0abdcdf5-16f1-4fbe-bc08-327f592a5c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidatorModel_1d4a1fdd6a27"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94fe73cc-5930-489b-be78-33c76ad1ca87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871545593662796\n",
      "0.9130434782608695\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.log.level\", \"error\")\n",
    "conf.set(\"spark.ui.showConsoleProgress\", \"false\") \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf) \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction au DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", True)\\\n",
    "               .option(\"inferSchema\", True)\\\n",
    "               .option(\"escape\", \"\\\"\")\\\n",
    "               .csv(\"penguins.csv\")\n",
    "\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "impute_mean = Imputer(strategy=\"mean\",\\\n",
    "                      inputCols=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"],\\\n",
    "                      outputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\"])\n",
    "\n",
    "impute_median = Imputer(strategy=\"median\",\\\n",
    "                        inputCol=\"body_mass_g\",\\\n",
    "                        outputCol=\"body_mass_g_f\")\n",
    "\n",
    "index = StringIndexer(inputCols=[\"species\", \"sex\", \"island\"],\\\n",
    "                      outputCols=[\"label\", \"sex_f\", \"island_f\"],\\\n",
    "                      handleInvalid=\"keep\")\n",
    "\n",
    "impute_mode = Imputer(strategy=\"mode\",\\\n",
    "                      missingValue=2.0,\\\n",
    "                      inputCol=\"sex_f\",\\\n",
    "                      outputCol=\"sex_f\")\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"island_f\",\\\n",
    "                        outputCol=\"island_encoded\")\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\", \"body_mass_g_f\"],\\\n",
    "                     outputCol=\"numerical_features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "final_va = VectorAssembler(inputCols=[\"island_encoded\", \"scaled_features\", \"sex_f\"],\\\n",
    "                           outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[impute_mean, impute_median, index, impute_mode, encoder, va, scaler, final_va, lr])\n",
    "\n",
    "pipelineModel = pipeline.fit(train)\n",
    "\n",
    "test_predict = pipelineModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "print(evaluator.evaluate(test_predict))\n",
    "print(evaluator.evaluate(test_predict, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f89f3f1-656c-4e10-a5f1-b3405a810fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, OneHotEncoder, StandardScaler, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.log.level\", \"error\")\n",
    "conf.set(\"spark.ui.showConsoleProgress\", \"false\") \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf) \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction au DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", True)\\\n",
    "               .option(\"inferSchema\", True)\\\n",
    "               .option(\"escape\", \"\\\"\")\\\n",
    "               .csv(\"penguins.csv\")\n",
    "\n",
    "train, test = df.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "impute_mean = Imputer(strategy=\"mean\",\\\n",
    "                      inputCols=[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"],\\\n",
    "                      outputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\"])\n",
    "\n",
    "impute_median = Imputer(strategy=\"median\",\\\n",
    "                        inputCol=\"body_mass_g\",\\\n",
    "                        outputCol=\"body_mass_g_f\")\n",
    "\n",
    "index = StringIndexer(inputCols=[\"species\", \"sex\", \"island\"],\\\n",
    "                      outputCols=[\"label\", \"sex_f\", \"island_f\"],\\\n",
    "                      handleInvalid=\"keep\")\n",
    "\n",
    "impute_mode = Imputer(strategy=\"mode\",\\\n",
    "                      missingValue=2.0,\\\n",
    "                      inputCol=\"sex_f\",\\\n",
    "                      outputCol=\"sex_f\")\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"island_f\",\\\n",
    "                        outputCol=\"island_encoded\")\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"bill_length_mm_f\", \"bill_depth_mm_f\", \"flipper_length_mm_f\", \"body_mass_g_f\"],\\\n",
    "                     outputCol=\"numerical_features\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"numerical_features\", outputCol=\"scaled_features\")\n",
    "\n",
    "final_va = VectorAssembler(inputCols=[\"island_encoded\", \"scaled_features\", \"sex_f\"],\\\n",
    "                           outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline = Pipeline(stages=[impute_mean, impute_median, index, impute_mode, encoder, va, scaler, final_va, lr])\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "output = ParamGridBuilder() \\\n",
    "    .baseOn({lr.labelCol: 'label'}) \\\n",
    "    .baseOn([lr.predictionCol, 'prediction']) \\\n",
    "    .addGrid(lr.regParam, [0.3, 1.3]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.6, 0.8]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline,\\\n",
    "                    estimatorParamMaps=output,\\\n",
    "                    evaluator=evaluator,\\\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "test_predict = cvModel.transform(test)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "\n",
    "print(evaluator.evaluate(test_predict))\n",
    "print(evaluator.evaluate(test_predict, {evaluator.metricName: \"accuracy\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c018e50-9a6e-4831-bd77-c782cd37d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
